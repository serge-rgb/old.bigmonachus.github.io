---
layout: post
categories: [devlog]
title: "A real time ray tracing renderer for the Oculus Rift"
---

# THIS POST IS A WORK IN PROGRESS. I WILL EDIT AND FINISH LATER TODAY

Real time ray tracing is about one of the coolest things you can work on. Add Virtual Reality to the mix and it just gets sweeter.


"Acid-RT" is a pet project of mine. It is a real time ray tracing renderer for the Oculus DK2 and it is [open source](http://github.com/bigmonachus/acid-rt).


[VIDEO]



Acid-RT is a wrapper around LibOVR. It operates at a higher level, acting as a graphics library for VR aplications. At this time, it is not suitable for general use, but it would be nice to get to that point.

This is not a post-mortem. I expect to continue working on this, but I feel that it has reached a point where I can write a good article and draw some conclusions.

## Part 1. Detailed Description.
<hr/>

## 1.1 - Interfacing with LibOVR

At initialization time, Acid-RT calls all the appropriate LibOVR functions and get all the necessary info to get going.

Acid-RT acts as a graphics library. It maintains state for all the geometry, and it handles everything at render time. The user needs to specify the scene data and the location and orientation of the camera.

In pseudocode:

```
func application_init():
    load_assets_and_setup()
    ACID_INIT(my_data)

func game_loop():
    do_my_thing():
    ACID_UPDATE(my_data)
    ACID_RENDER()
```

This project would not be possible if Oculus did not release the source code for LibOVR. It is a Good Thing that they do release it.

LibOVR has become increasingly complex. With DK1, little more was needed than a cleary defined cubic polynomial to do barrel distortion and a quaternion describing the predicted head position. With DK2, things got a lot more complex, and writing each VR feature required figuring out how it worked for LibOVR, and then writing the equivalent code for the ray tracing renderer.

The standard way to integrate LibOVR into an engine involves creating a distortion mesh and setting up textures. Each frame, the engine is supposed to give LibOVR a framebuffer for each eye and ignore how the rest of the sausage gets made.

In the name of elegance, Acid-RT tries to avoid calling complex initialization functions as much as possible. It is necessary to give LibOVR access to the GL context and to initialize the system so that we can obtain position information, as well as physical quantities such as how far away are the lens set from the eye and the corresponding distortion parameters, also determined by the type of lens hooked to the Rift.

## 1.2 - Lens distortion correction

Lens distortion is the first and most important challenge to solve. For performance reasons, LibOVR moved from a simple pixel shader in DK1 to a more complex mesh-based approach. Fortunately, after some digging, we can find out that the mesh is morphed by an equation, just as the DK1 had a simple cubic barrel distortion equation. At the time of this writing, it is an 10 point spline defined by 11 coefficients. If you are interested, you can google for "Cubic Hermite spline" and look for the definition of the Catmull-Rom spline.

Once we have a way to compute the spline from the 11 coefficients that define it, we can compute the distortion on a ray-by-ray basis. This is one place where ray tracing leads to more elegant solutions, by evaluating the spline, we are essentially bending the rays just as a corrective lens would. More on this later.

## 1.3 - Bounding Volume Hierarchies

Acid-RT stores geometry data as "primitives": chunks of adjacent triangles. When loading meshes, a function called "shatter" builds an octree from the loaded vertex data and outputs an array of primitives, obtained from octree cells.

The renderer stores a big soup of primitives. All knowledge of different entities is lost at the "primitive soup" level.

This "primitive soup" is used as input to construct a [BVH](http://en.wikipedia.org/wiki/Bounding_volume_hierarchy). This acceleration structure is by far the most popular and most researched structure in the real time ray tracing community. The method I use to build the BVH is the one you can find in the book "Physically Based Rendering", using the Surface Area Heuristic to create good trees.

It is generally considered that SBVH (Spatial Bounding Volume Hierarchy. Paper [here](http://www.nvidia.com/docs/IO/77714/sbvh.pdf)) is a more efficient way to construct a tree. It builds BVH trees in the usual way but it does some work to reduce overlapping of bounding boxes.

As I said, when Acid-RT loads meshes, it shatters them into primitives using an octree. This "octree shatter" method results in zero-overlap between primitives, which is the goal of SBVH. This means that I may have stumbled into a good alternative to SBVH, but I have yet to confirm this. I was happy to learn that the "octree shatter" algorithm has a name, but I don't remember it right now.

## 1.4 - GL Compute vs OpenCL

The first implementation was based on OpenGL compute shaders. Compute shaders have the advantage over OpenCL or CUDA that they are guaranteed to be available if OpenGL 4.3 is supported; but I had to give them up because I wasn't getting the performance I wanted, and because on more than a couple of occasions I ran into shader compiler bugs.

I migrated to OpenCL and immediately got a about 5x speed boost. I got a lot of extra speed by tweaking for performance and experimenting with different traversal algorithms.

## 1.5 - Traversal algorithm

The most critical part of the ray tracer is the BVH traversal code. Most people who know what they are doing swear by this [fantastic paper](https://code.google.com/p/understanding-the-efficiency-of-ray-traversal-on-gpus/). Acid-RT uses a traversal algorithm based on this paper. I have done many optimizations to my implementation, but they all have been educated guesses followed by measuring. In order to speed things up effectively, I need to do actual profiling. Based on the paper I linked to and a back of the envelope calculation, I estimate that the renderer is a factor of 4 or 5 away from getting all the performance it can squeeze out of my GPU, more on this later.

The Compute ray tracer was limited by the stack size, so ofter migrating to OpenCL, one thing I tried was to implement stackless traversal ([paper here](http://onlinelibrary.wiley.com/doi/10.1111/cgf.12259/pdf)). It was an unsuccessful approach. It is much slower than the equivalent traversal algorithm that does maintain a stack. Almost certainly because of many more memory accesses required.

I tried both the `if-if` and the `while-while` traversal methods described in the super-paper. `While-while` is much faster.

The world moves quickly. On my GPU, a GTX770, using MAD(multiply-add) intrinsics gives no measurable difference. A lot of presentations I stumbled upon suggested explicit MADs as an optimization technique.

## 1.6 - Anti-aliasing

Ray tracing is expensive, doubly so with VR. Multi-sampling is off the table. Acid-RT is currently using Screen Space Anti Aliasing. I took Timothy Lotte's FXAA code and used it for this. It works OK for smooth edges, but there is still a huge problem with sub-pixel aliasing. I have no solution for this other than multi-sampling.

[PICTURE]

_Sub pixel aliasing_

## 1.7 - Chromatic aberration correction

[PICTURE]

The way Acid-RT deals with chromatic aberration is by scaling each color component by a factor of the distance from the center of the lens. These 3 factors, one for R, G, B respectively, were determined "heuristically", which is a fancy way of saying that I tweaked them until things looked OK. This approach works pretty well. It works at least as well as what is currently done by LibOVR.

## 1.8 - Time warp

The the vr layer in Acid-RT provides the `begin_frame` and `end_frame` functions. Among other things, `end_frame` returns time warp matrices.

Time warp is a rotation done after the frame has been rendered but before vsync. It represents the difference in head orientation from the start of the frame and the point when it is displayed on the screen. For Acid-RT, the rendered frame is a quad that is rendered by a simple shader. As simple as it gets. The implementation of timewarp is to just rotate the quad (with z=1) by the TimeWarp matrix.

# Part 2. GC and Crazy ideas
<hr/>
I started this project with garbage collection, using the Boehm GC. It gave me a lot of problems and I ended up ditching it almost completely.

The first problem was stability. I found a bug, and then found out that it had been reported about 10 years ago.
The other problem was obfuscation. Usually, simple mistakes with memory management come up immediately. With Boehm GC, they show up as difficult to understand errors deep within the codebase. Whenever that happened, I had to go up the call stack to see my mistake, instead of getting a nice friendly access violation crash.

I did a lot of crazy experiments. One of them was to divide the screen in a checker board pattern and, each frame, only draw either the odd squares or the even squares. The idea was to render half the pixels without reducing the resolution. This doubled the speed but caused very ugly artifacts. The experiment was more successful when I moved from squares to thin horizontal rectangles. The performance gain was not as dramatic, but it was not horrible. It looks kind of like the artifacs you see when you convert interlaced video to digital formats. It used to be very common.

Right now I am rendering to a smaller than 1080p backbuffer, and then resampling to 1080p. With linear resampling this doesn't look too bad and it gives the 2x boost that I currently need. I hope that a nicer resampling would make this a viable way to do things.

# Part 3. Future work.
<hr/>

* The next thing I plan to do is to switch to direct mode. Right now Acid-RT works only in "extended-mode", which means the Rift is acting as a secondary display. This reportedly gives at least one frame of extra latency, but I haven't measured it. I see the similar latency to my renderer when I play something like Elite: Dangerous, which runs on extended-mode. I would really like my renderer to run as perfectly as the Oculus samples.

* I need to switch to CUDA, even if that means supporting NVidia only. Like Mike Acton said, different hardware means different problem. OpenCL is too absracted away and there are no useful tools for me to speed up the ray tracer. I need CUDA to break that gap between the performance I am getting and the theoretical limit.

* Right now, OpenCL and OpenGL are sharing the backbuffer texture. By being less sophisticated and disabling sharing, asynchronous timewarp would be relatively easy to implement.

* I want to return to the checkerboard experiment, and use two-frame timewarp matrices. Fresh tiles get one-frame timewarped, and old tiles get two-frame timewarped. This would reduce the artifacts, hopefully by a lot. Probably fun to try out.

* I want to write a simple program that is bigger than a sample but smaller than a game, so that I can implement features like dynamic lighting and animation the right way.

# Part 4. Conclusions
<hr/>

There is no practical reason to use ray tracing instead of rasterization for VR applications. The advantage of ray tracing is that most problems become simpler, with the notable exception of dynamic scenes. The end-user will not directly benefit from a tidier, prettier codebase, and the cases where ray tracing benefits are visible to the user probably won't show up in something as demanding as VR.

That said, I can see a simple game implemented with something like what I have here. It might consume more resources than necessary, but as long as it runs at more than 75 or 90 FPS then no one will get hurt; and the programmer's soul won't be as damaged from OpenGL exposure.

I expected lens distortion to get simpler with ray tracing, but chromatic aberration was also simpler and time warp was much simpler. Async timewarp seems to be an easy thing to add, which is definitely not the case for the traditional way.

There is no need to draw to a buffer with twice the dimensions so that it can wrap gracefully with distortion. In fact, I am drawing to a small backbuffer. This is not possible with the traditional way without very ugly consequences.



